{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Iana_Kasimova/bert_ner/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, Features, ClassLabel, Value\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have text in json files, we can reach text by using id of document from csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In csv we can see labels, separated '|' from the document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>label_count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007f880-0a9b-492d-9a58-76eb0b0e0bd7</td>\n",
       "      <td>1</td>\n",
       "      <td>program for the international assessment of ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0008656f-0ba2-4632-8602-3017b44c2e90</td>\n",
       "      <td>1</td>\n",
       "      <td>trends in international mathematics and scienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000e04d6-d6ef-442f-b070-4309493221ba</td>\n",
       "      <td>1</td>\n",
       "      <td>agricultural resources management survey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000efc17-13d8-433d-8f62-a3932fe4f3b8</td>\n",
       "      <td>2</td>\n",
       "      <td>adni|alzheimer s disease neuroimaging initiati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0010357a-6365-4e5f-b982-582e6d32c3ee</td>\n",
       "      <td>1</td>\n",
       "      <td>genome sequence of covid 19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id  label_count  \\\n",
       "0  0007f880-0a9b-492d-9a58-76eb0b0e0bd7            1   \n",
       "1  0008656f-0ba2-4632-8602-3017b44c2e90            1   \n",
       "2  000e04d6-d6ef-442f-b070-4309493221ba            1   \n",
       "3  000efc17-13d8-433d-8f62-a3932fe4f3b8            2   \n",
       "4  0010357a-6365-4e5f-b982-582e6d32c3ee            1   \n",
       "\n",
       "                                               label  \n",
       "0  program for the international assessment of ad...  \n",
       "1  trends in international mathematics and scienc...  \n",
       "2           agricultural resources management survey  \n",
       "3  adni|alzheimer s disease neuroimaging initiati...  \n",
       "4                        genome sequence of covid 19  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"bert-base-cased\"\n",
    "train_path = \"../../data/coleridgeinitiative-show-us-the-data/train/\"\n",
    "test_path = \"../../data/coleridgeinitiative-show-us-the-data/test/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "df = pd.read_csv(\"../../data/coleridgeinitiative-show-us-the-data/train.csv\")\n",
    "\n",
    "df = df.groupby(['Id']).agg(label_count=('cleaned_label', 'count'),\n",
    "label = ('cleaned_label', '|'.join)).reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(df, path):\n",
    "    result = df.copy()\n",
    "    for i, id in enumerate(df.Id):\n",
    "        location = f\"{path}{id}.json\"\n",
    "        if not os.path.exists(location):\n",
    "            result.drop(i, inplace=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = filter_dataset(df, test_path)\n",
    "train_df = filter_dataset(df, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>label_count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td>2</td>\n",
       "      <td>adni|alzheimer s disease neuroimaging initiati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2624</th>\n",
       "      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n",
       "      <td>1</td>\n",
       "      <td>trends in international mathematics and scienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n",
       "      <td>2</td>\n",
       "      <td>slosh model|noaa storm surge inundation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7937</th>\n",
       "      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n",
       "      <td>1</td>\n",
       "      <td>rural urban continuum codes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Id  label_count  \\\n",
       "1826  2100032a-7c33-4bff-97ef-690822c43466            2   \n",
       "2624  2f392438-e215-4169-bebf-21ac4ff253e1            1   \n",
       "3499  3f316b38-1a24-45a9-8d8c-4e05a42257c6            2   \n",
       "7937  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60            1   \n",
       "\n",
       "                                                  label  \n",
       "1826  adni|alzheimer s disease neuroimaging initiati...  \n",
       "2624  trends in international mathematics and scienc...  \n",
       "3499            slosh model|noaa storm surge inundation  \n",
       "7937                        rural urban continuum codes  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all texts from json file for one ID from read_all_json() file\n",
    "def data_joining(all_texts):\n",
    "    data_length = len(all_texts)\n",
    "\n",
    "    temp = [all_texts[i]['text'] for i in range(data_length)]\n",
    "    temp = '. '.join(temp)\n",
    "\n",
    "    return temp\n",
    "\n",
    "def read_all_json(df, path):\n",
    "    text_data = []\n",
    "    for i, rec_id in tqdm(enumerate(df.Id), total=df.shape[0]):\n",
    "        location = f\"{path}{rec_id}.json\"\n",
    "        if os.path.exists(location):\n",
    "            \n",
    "            with open(location, 'r') as f:\n",
    "                text_dict = {\"id\": rec_id, \"text\": data_joining(json.load(f)), \"label\": df.label[i]}\n",
    "                text_data.append(text_dict)\n",
    "    print(\"All files were read\") \n",
    "\n",
    "    return text_data \n",
    "\n",
    "def read_json(id, path):\n",
    "    location = f\"{path}{id}.json\"\n",
    "    if os.path.exists(location):\n",
    "        with open(location, 'r') as f:\n",
    "            return data_joining(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n",
    "\n",
    "def make_shorter_sentence(sentence, max_length):\n",
    "    sent_tokenized = sent_tokenize(sentence)\n",
    "\n",
    "    final_sentences = []\n",
    "\n",
    "    for tok_sent in sent_tokenized:\n",
    "        tok_sent_clean = clean_text(tok_sent)\n",
    "        tok_sent_clean = tok_sent_clean.replace(\".\", \"\").rstrip()\n",
    "\n",
    "        tok_sent = tok_sent_clean.split(\" \")\n",
    "\n",
    "        while len(tok_sent) > max_length:\n",
    "            final_sentences.append(\" \".join(tok_sent[0:max_length]))\n",
    "            tok_sent = tok_sent[max_length:]\n",
    "    \n",
    "        final_sentences.append(\" \".join(tok_sent))\n",
    "\n",
    "    return final_sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling\n",
    "BIO tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " # get sentence, divide on a smaller sentence if it needs, and labeled each sentence by BIO tagging\n",
    "#B-0\n",
    "#I-1\n",
    "#O-2\n",
    "# For each sentence we look does it contain the labels or not, if sentence very long we separate it on a small sentences \n",
    "\n",
    "def convert_to_labels_iter(tokenizer, text, labels, max_length):\n",
    "    tok_sent = make_shorter_sentence(text, max_length)\n",
    "    items = []\n",
    "    for sent in tok_sent:\n",
    "        # here we can have more tokens than we have words in sentence\n",
    "        # padding=False by default  \n",
    "        tokenized_inputs = tokenizer([sent], add_special_tokens = True, truncation=True, is_split_into_words=True)\n",
    "        item = {k:torch.tensor(v) for k, v in tokenized_inputs.items()}\n",
    "        word_ids = tokenized_inputs[\"input_ids\"]\n",
    "        s = np.array([2] * len(word_ids))\n",
    "        # set [CLS] and [SEP] tokens = -100\n",
    "        s[0] = -100\n",
    "        s[len(word_ids) - 1] = -100\n",
    "        for label in labels:\n",
    "            if label in sent:\n",
    "                # here we can have more tokens than we have words in sentence\n",
    "                labeled_inputs = tokenizer([label], add_special_tokens = False, truncation=True, is_split_into_words=True)\n",
    "                label_ids = labeled_inputs[\"input_ids\"]\n",
    "                for idx in range(len(word_ids)):\n",
    "                    if word_ids[idx:idx + len(label_ids)] == label_ids:\n",
    "                        s[idx] = 0\n",
    "                        if len(label_ids) > 1:\n",
    "                            s[idx+1: idx + len(label_ids)] = 1\n",
    "        item[\"labels\"] = torch.tensor(s)\n",
    "        items.append(item)\n",
    "    return items                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomIterableDataset(torch.utils.data.IterableDataset):\n",
    "    # encodings it is our tokenizer\n",
    "    def __init__(self, ids, labels, encodings, convert_to_labels, read_json, path, max_length):\n",
    "        self.encodings = encodings\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "        self.max_length = max_length\n",
    "        self.path = path\n",
    "        self.reader = read_json\n",
    "        self.converter = convert_to_labels\n",
    "\n",
    "    # provide dictionary with attention_mask, labels and input_ids\n",
    "    def __iter__(self):\n",
    "        iter_items = []\n",
    "        for id, labels in zip(self.ids, self.labels):\n",
    "        #read from the disc by id\n",
    "            text = self.reader(id, self.path)\n",
    "            items = self.converter(self.encodings, text, labels.split(\"|\"), self.max_length)\n",
    "            iter_items.extend(items)\n",
    "        return iter(iter_items) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels, label_names):\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [[label_names[p] for p, l in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "0it [00:00, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "27it [01:25,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.9988370863727667}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [01:21,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.9988370863727667}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import DistilBertForTokenClassification, AdamW, DataCollatorForTokenClassification, get_scheduler\n",
    "from transformers import AutoTokenizer\n",
    "import evaluate\n",
    "\n",
    "train_df_short = train_df.head(2)\n",
    "max_length=128\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# fill -100 tensors till the max length in a batch\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "train_dataset = CustomIterableDataset(train_df_short.Id.values, train_df_short.label.values, tokenizer, convert_to_labels_iter, read_json, train_path, max_length)\n",
    "test_dataset = CustomIterableDataset(test_df.Id.values, test_df.label.values, tokenizer, convert_to_labels_iter, read_json, test_path, max_length)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, collate_fn = data_collator,  batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, collate_fn = data_collator,  batch_size=16)\n",
    "\n",
    "labels_names = ['B', 'I', 'O']\n",
    "epoch_num= 2\n",
    "\n",
    "model = DistilBertForTokenClassification.from_pretrained('distilbert-base-uncased', num_labels=len(labels_names))\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        optim.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    for batch in test_loader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]    \n",
    "        true_predictions, true_labels = postprocess(predictions, labels, labels_names)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "    results = metric.compute()    \n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
